{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99a3a602",
   "metadata": {},
   "source": [
    "# Language Model Fine-tuning with Hyperparameter Optimization\n",
    "\n",
    "This notebook demonstrates how to fine-tune a DistilGPT-2 model using Ray Tune for automated hyperparameter optimization. We will:\n",
    "\n",
    "1. Install required packages\n",
    "2. Set up the model and tokenizer\n",
    "3. Prepare the dataset\n",
    "4. Define training functions\n",
    "5. Configure hyperparameter search\n",
    "6. Run automated optimization\n",
    "7. Analyze results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282e2320",
   "metadata": {},
   "source": [
    "## Step 1: Install Required Packages\n",
    "\n",
    "Install the necessary packages for language model fine-tuning and hyperparameter optimization:\n",
    "- `transformers`: Hugging Face library for pre-trained models\n",
    "- `datasets`: For loading and processing datasets\n",
    "- `accelerate`: For distributed training support\n",
    "- `ray[tune]`: For hyperparameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9706f5b0-75fc-418f-a54a-f03c07a9c083",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/igraph-0.11.8-py3.12-linux-x86_64.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/texttable-1.7.0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/looseversion-1.3.0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/opt_einsum-3.4.0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/lightning_utilities-0.11.9-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/nvfuser-0.2.13a0+0d33366-py3.12-linux-x86_64.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/lightning_thunder-0.2.0.dev0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Install required packages with quiet flag to reduce output verbosity\n",
    "# transformers: for pre-trained language models and tokenizers\n",
    "# datasets: for loading and processing machine learning datasets\n",
    "# accelerate: for efficient distributed training\n",
    "# ray[tune]: for automated hyperparameter optimization\n",
    "!pip install transformers datasets accelerate ray[tune] --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c9a687",
   "metadata": {},
   "source": [
    "## Step 2: Import Libraries and Setup\n",
    "\n",
    "Import all necessary libraries and set up the computing device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0e5623",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import PyTorch for deep learning operations\n",
    "import torch\n",
    "\n",
    "# Import Hugging Face transformers components for language modeling\n",
    "from transformers import (\n",
    "    AutoTokenizer,                    # For text tokenization\n",
    "    AutoModelForCausalLM,            # For causal language models like GPT\n",
    "    Trainer,                         # For training loop management\n",
    "    TrainingArguments,               # For training configuration\n",
    "    DataCollatorForLanguageModeling  # For batch preparation during training\n",
    ")\n",
    "\n",
    "# Import datasets library for loading and processing data\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Import Ray for distributed computing and hyperparameter optimization\n",
    "import ray\n",
    "from ray import tune, train\n",
    "from ray.tune.schedulers import ASHAScheduler  # Asynchronous Successive Halving Algorithm\n",
    "\n",
    "# Determine the computing device (GPU if available, otherwise CPU)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00cbfd7",
   "metadata": {},
   "source": [
    "## Step 3: Model and Tokenizer Setup\n",
    "\n",
    "Define a function to load and configure the DistilGPT-2 model and its tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16aabb25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to initialize model and tokenizer\n",
    "def get_model_and_tokenizer():\n",
    "    # Load pre-trained DistilGPT-2 tokenizer (smaller version of GPT-2)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\")\n",
    "    \n",
    "    # Set padding token to be the same as end-of-sequence token\n",
    "    # This is necessary because GPT-2 doesn't have a dedicated padding token\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    # Load the pre-trained DistilGPT-2 model for causal language modeling\n",
    "    # Causal LM means the model predicts the next token given previous tokens\n",
    "    model = AutoModelForCausalLM.from_pretrained(\"distilgpt2\")\n",
    "    \n",
    "    # Resize token embeddings to match tokenizer vocabulary size\n",
    "    # This ensures compatibility between model and tokenizer\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    \n",
    "    # Move model to the appropriate device (GPU/CPU) and return both components\n",
    "    return model.to(device), tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e201b64",
   "metadata": {},
   "source": [
    "## Step 4: Dataset Preparation\n",
    "\n",
    "Define a function to load and tokenize the WikiText-2 dataset for language model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7accc23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to load and preprocess the dataset\n",
    "def load_tokenized_dataset(tokenizer, block_size=64):\n",
    "    # Load WikiText-2 dataset (a collection of Wikipedia articles)\n",
    "    # Using only 1% of the training split for faster experimentation\n",
    "    raw_dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train[:1%]\")\n",
    "\n",
    "    # Define tokenization function for the dataset\n",
    "    def tokenize(example):\n",
    "        # Tokenize the text with padding and truncation\n",
    "        # max_length=block_size limits sequence length for memory efficiency\n",
    "        # padding=\"max_length\" ensures all sequences have the same length\n",
    "        # truncation=True cuts off text that exceeds max_length\n",
    "        tokens = tokenizer(\n",
    "            example[\"text\"], \n",
    "            padding=\"max_length\", \n",
    "            truncation=True, \n",
    "            max_length=block_size\n",
    "        )\n",
    "        \n",
    "        # For causal language modeling, labels are the same as input_ids\n",
    "        # The model learns to predict the next token in the sequence\n",
    "        tokens[\"labels\"] = tokens[\"input_ids\"].copy()\n",
    "        \n",
    "        return tokens\n",
    "\n",
    "    # Apply tokenization to the entire dataset\n",
    "    # batched=True processes multiple examples at once for efficiency\n",
    "    # remove_columns=[\"text\"] removes the original text column after tokenization\n",
    "    tokenized = raw_dataset.map(tokenize, batched=True, remove_columns=[\"text\"])\n",
    "    \n",
    "    # Set format to PyTorch tensors for compatibility with PyTorch models\n",
    "    tokenized.set_format(\"torch\")\n",
    "    \n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c435dc5",
   "metadata": {},
   "source": [
    "## Step 5: Training Function for Ray Tune\n",
    "\n",
    "Define the training function that will be optimized by Ray Tune. This function receives hyperparameter configurations and reports back the evaluation loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e33e5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training function that Ray Tune will call with different hyperparameter configurations\n",
    "def train_with_tune(config):\n",
    "    # Print the current hyperparameter configuration being tested\n",
    "    print(f\"Starting trial with config: {config}\")\n",
    "    \n",
    "    try:\n",
    "        # Initialize model and tokenizer for this trial\n",
    "        model, tokenizer = get_model_and_tokenizer()\n",
    "        \n",
    "        # Load and tokenize the dataset\n",
    "        dataset = load_tokenized_dataset(tokenizer)\n",
    "\n",
    "        # Split dataset into training and evaluation sets (80/20 split)\n",
    "        train_size = int(0.8 * len(dataset))\n",
    "        train_dataset = dataset.select(range(train_size))\n",
    "        eval_dataset = dataset.select(range(train_size, len(dataset)))\n",
    "\n",
    "        # Print dataset sizes for monitoring\n",
    "        print(f\"Train size: {len(train_dataset)}, Eval size: {len(eval_dataset)}\")\n",
    "\n",
    "        # Configure training arguments using hyperparameters from Ray Tune\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=\"./output\",                                    # Directory to save model checkpoints\n",
    "            per_device_train_batch_size=config[\"batch_size\"],        # Batch size from hyperparameter search\n",
    "            learning_rate=config[\"lr\"],                              # Learning rate from hyperparameter search\n",
    "            num_train_epochs=config[\"epochs\"],                       # Number of epochs from hyperparameter search\n",
    "            logging_steps=5,                                         # Log metrics every 5 steps\n",
    "            save_strategy=\"no\",                                      # Don't save checkpoints to save disk space\n",
    "            report_to=\"none\",                                        # Disable external logging (like wandb)\n",
    "            fp16=torch.cuda.is_available(),                          # Use half-precision if GPU is available\n",
    "        )\n",
    "\n",
    "        # Create data collator for language modeling\n",
    "        # mlm=False indicates causal language modeling (like GPT), not masked language modeling (like BERT)\n",
    "        data_collator = DataCollatorForLanguageModeling(\n",
    "            tokenizer=tokenizer, \n",
    "            mlm=False  # GPT-style causal LM, not masked LM\n",
    "        )\n",
    "\n",
    "        # Initialize the Trainer with model, arguments, datasets, and data collator\n",
    "        trainer = Trainer(\n",
    "            model=model,                    # The model to train\n",
    "            args=training_args,             # Training configuration\n",
    "            train_dataset=train_dataset,    # Training data\n",
    "            eval_dataset=eval_dataset,      # Evaluation data\n",
    "            data_collator=data_collator,    # Handles batch preparation\n",
    "        )\n",
    "\n",
    "        # Execute the training process\n",
    "        trainer.train()\n",
    "\n",
    "        # Evaluate the model on the validation set\n",
    "        result = trainer.evaluate()\n",
    "        print(\"Eval results:\", result)\n",
    "\n",
    "        # Extract evaluation loss for Ray Tune optimization\n",
    "        eval_loss = result.get(\"eval_loss\", None)\n",
    "        \n",
    "        # Handle case where evaluation loss is missing\n",
    "        if eval_loss is None:\n",
    "            print(\"Warning: eval_loss missing. Reporting dummy loss = 9999.\")\n",
    "            eval_loss = 9999.0\n",
    "\n",
    "        # Report the loss back to Ray Tune for optimization\n",
    "        # Ray Tune will use this to determine which hyperparameters work best\n",
    "        train.report({\"loss\": eval_loss})\n",
    "\n",
    "    except Exception as e:\n",
    "        # Handle any errors during training by reporting a high loss value\n",
    "        print(f\"Trial crashed: {e}\")\n",
    "        train.report({\"loss\": 9999.0})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35103838",
   "metadata": {},
   "source": [
    "## Step 6: Configure Hyperparameter Search Space\n",
    "\n",
    "Define the hyperparameters to optimize and their possible values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35642e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hyperparameter search space for optimization\n",
    "search_space = {\n",
    "    # Batch size options: small values for memory efficiency\n",
    "    # tune.choice() randomly selects one of the provided options\n",
    "    \"batch_size\": tune.choice([1, 2]),\n",
    "    \n",
    "    # Learning rate range: logarithmic distribution between 1e-5 and 1e-4\n",
    "    # tune.loguniform() samples from a log-uniform distribution\n",
    "    # This is common for learning rates as they often work best on a log scale\n",
    "    \"lr\": tune.loguniform(1e-5, 1e-4),\n",
    "    \n",
    "    # Number of training epochs: limited to 1 or 2 for fast experimentation\n",
    "    # In practice, you might use more epochs for better convergence\n",
    "    \"epochs\": tune.choice([1, 2]),\n",
    "}\n",
    "\n",
    "# Initialize ASHA (Asynchronous Successive Halving Algorithm) scheduler\n",
    "# This scheduler stops poorly performing trials early to save computational resources\n",
    "# It allocates more resources to promising hyperparameter configurations\n",
    "scheduler = ASHAScheduler()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973bc3ee",
   "metadata": {},
   "source": [
    "## Step 7: Initialize Ray and Run Hyperparameter Optimization\n",
    "\n",
    "Initialize Ray for distributed computing and execute the hyperparameter search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c98633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean shutdown of any existing Ray instance to avoid conflicts\n",
    "ray.shutdown()\n",
    "\n",
    "# Initialize Ray for distributed computing\n",
    "# ignore_reinit_error=True prevents errors if Ray is already running\n",
    "# num_cpus=2 limits CPU usage for this notebook environment\n",
    "ray.init(ignore_reinit_error=True, num_cpus=2)\n",
    "\n",
    "# Execute the hyperparameter optimization using Ray Tune\n",
    "analysis = tune.run(\n",
    "    train_with_tune,                                          # Function to optimize\n",
    "    config=search_space,                                      # Hyperparameter search space\n",
    "    num_samples=2,                                           # Number of different configurations to try\n",
    "    scheduler=scheduler,                                      # ASHA scheduler for early stopping\n",
    "    metric=\"loss\",                                           # Metric to optimize (minimize loss)\n",
    "    mode=\"min\",                                              # Minimize the metric (lower loss is better)\n",
    "    resources_per_trial={                                    # Resource allocation per trial\n",
    "        \"cpu\": 1,                                            # 1 CPU core per trial\n",
    "        \"gpu\": 0.5 if torch.cuda.is_available() else 0      # 0.5 GPU per trial if available\n",
    "    },\n",
    "    raise_on_failed_trial=False,                             # Continue even if some trials fail\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531587d9",
   "metadata": {},
   "source": [
    "## Step 8: Analyze Optimization Results\n",
    "\n",
    "Extract and display the best hyperparameter configuration found by Ray Tune."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3262ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze and display the optimization results\n",
    "if analysis.best_config:\n",
    "    # If successful trials were found, display the best hyperparameter configuration\n",
    "    print(\"Best hyperparameters found:\", analysis.best_config)\n",
    "    \n",
    "    # You can also access additional information about the best trial:\n",
    "    # analysis.best_trial - the best trial object\n",
    "    # analysis.best_result - the best result metrics\n",
    "    # analysis.best_logdir - directory containing the best trial's logs\n",
    "    \n",
    "else:\n",
    "    # If no successful trials were completed, inform the user\n",
    "    print(\"No successful trials completed. The model setup is ready for manual tuning.\")\n",
    "\n",
    "# Additional analysis you can perform:\n",
    "# analysis.trials - list of all trials\n",
    "# analysis.get_best_trial() - get the best trial object\n",
    "# analysis.results_df - pandas DataFrame with all results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
