{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z2jeIdz9_m0o",
        "outputId": "711633e8-c21b-4013-b9ae-5b985c83f7ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Libraries installed and imported successfully.\n",
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at Salesforce/codegen-350M-mono were not used when initializing CodeGenForCausalLM: ['transformer.h.0.attn.causal_mask', 'transformer.h.1.attn.causal_mask', 'transformer.h.10.attn.causal_mask', 'transformer.h.11.attn.causal_mask', 'transformer.h.12.attn.causal_mask', 'transformer.h.13.attn.causal_mask', 'transformer.h.14.attn.causal_mask', 'transformer.h.15.attn.causal_mask', 'transformer.h.16.attn.causal_mask', 'transformer.h.17.attn.causal_mask', 'transformer.h.18.attn.causal_mask', 'transformer.h.19.attn.causal_mask', 'transformer.h.2.attn.causal_mask', 'transformer.h.3.attn.causal_mask', 'transformer.h.4.attn.causal_mask', 'transformer.h.5.attn.causal_mask', 'transformer.h.6.attn.causal_mask', 'transformer.h.7.attn.causal_mask', 'transformer.h.8.attn.causal_mask', 'transformer.h.9.attn.causal_mask']\n",
            "- This IS expected if you are initializing CodeGenForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing CodeGenForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model 'Salesforce/codegen-350M-mono' loaded successfully.\n",
            "Code generation function is defined.\n",
            "Evaluation helper functions are defined.\n",
            "\n",
            "Starting evaluation on the HumanEval dataset...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 164/164 [12:57<00:00,  4.74s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "EVALUATION COMPLETE\n",
            "==================================================\n",
            "Total problems evaluated: 164\n",
            "Problems solved correctly: 150\n",
            "pass@1 Score: 91.46%\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# ==============================================================================\n",
        "# 1. SETUP AND INSTALLATION\n",
        "# ==============================================================================\n",
        "\n",
        "# Install the essential libraries using pip. The '-q' flag ensures a quiet installation.\n",
        "# - transformers: Provides the core architecture (like AutoModelForCausalLM) and tools to work with LLMs.\n",
        "# - torch: The backend tensor library for model computations.\n",
        "# - datasets: A convenient library for downloading and using standard evaluation benchmarks like HumanEval.\n",
        "# - accelerate: Helps in efficiently loading and running large models on available hardware, like a GPU.\n",
        "!pip install transformers torch datasets accelerate -q\n",
        "\n",
        "# Import the necessary classes and modules for the script.\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from datasets import load_dataset\n",
        "import re  # The regular expression module, used for cleaning up generated code.\n",
        "from tqdm import tqdm  # A utility for creating smart, descriptive progress bars for loops.\n",
        "\n",
        "print(\"Libraries installed and imported successfully.\")\n",
        "\n",
        "# ==============================================================================\n",
        "# 2. MODEL AND TOKENIZER INITIALIZATION\n",
        "# ==============================================================================\n",
        "\n",
        "# Set the computational device. We check if a CUDA-enabled GPU is available\n",
        "# (which is standard in Colab T4 runtimes) and fall back to the CPU if not.\n",
        "# Running on a GPU is significantly faster for model inference.\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Define the model checkpoint from the Hugging Face Hub.\n",
        "# 'Salesforce/codegen-350M-mono' is a 350-million parameter model specifically\n",
        "# trained on Python code ('mono' means single-language). It's \"non-gated,\"\n",
        "# meaning we can use it without needing to log in or accept special terms.\n",
        "checkpoint = \"Salesforce/codegen-350M-mono\"\n",
        "\n",
        "# Load the tokenizer associated with the chosen model. The tokenizer is responsible\n",
        "# for converting human-readable text into a sequence of numerical tokens that the model understands.\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "\n",
        "# Load the pre-trained model itself.\n",
        "# - torch_dtype=torch.bfloat16: This loads the model weights in a 16-bit format,\n",
        "#   which uses half the memory of standard 32-bit floats, making it run faster\n",
        "#   and fit more easily into the GPU's memory without a significant loss in accuracy.\n",
        "# - .to(device): This crucial step moves the entire model onto the selected device (the GPU).\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    checkpoint,\n",
        "    torch_dtype=torch.bfloat16,\n",
        ").to(device)\n",
        "\n",
        "print(f\"Model '{checkpoint}' loaded successfully.\")\n",
        "\n",
        "# ==============================================================================\n",
        "# 3. CODE GENERATION FUNCTION\n",
        "# ==============================================================================\n",
        "\n",
        "def generate_code(prompt: str, max_new_tokens: int = 150) -> str:\n",
        "    \"\"\"\n",
        "    Generates a code completion for a given prompt using the loaded LLM.\n",
        "    \"\"\"\n",
        "    # First, the tokenizer converts the input string `prompt` into numerical tensors.\n",
        "    # .to(device) moves these tensors to the GPU to be processed by the model.\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    # Call the model's generate method to perform inference.\n",
        "    # - **inputs: Unpacks the tokenized input tensors.\n",
        "    # - max_new_tokens: Limits the length of the generated code to prevent overly long or rambling outputs.\n",
        "    # - temperature=0.2: Controls randomness. A lower value makes the output more deterministic and focused.\n",
        "    # - top_p=0.95: Nucleus sampling, which considers only the most probable tokens that make up 95% of the probability mass.\n",
        "    # - do_sample=True: Enables sampling-based generation, which is necessary for temperature and top_p to have an effect.\n",
        "    # - pad_token_id: Sets the padding token to the \"end-of-sentence\" token, a common practice.\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        temperature=0.2,\n",
        "        top_p=0.95,\n",
        "        do_sample=True,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "    # The model's output is a tensor of token IDs. The tokenizer's decode method\n",
        "    # converts these IDs back into a human-readable string.\n",
        "    full_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    # The generated output includes our original prompt. We slice the string\n",
        "    # to return only the newly generated code that comes after the prompt.\n",
        "    return full_text[len(prompt):]\n",
        "\n",
        "print(\"Code generation function is defined.\")\n",
        "\n",
        "# ==============================================================================\n",
        "# 4. EVALUATION FRAMEWORK\n",
        "# ==============================================================================\n",
        "\n",
        "# Load the HumanEval dataset from the Hugging Face Hub. This dataset consists of\n",
        "# 164 programming problems, each with a prompt, a canonical solution, and unit tests.\n",
        "human_eval_dataset = load_dataset(\"openai_humaneval\")\n",
        "\n",
        "def create_full_script(prompt: str, generated_code: str) -> str:\n",
        "    \"\"\"\n",
        "    Cleans the generated code and combines it with the original prompt\n",
        "    to create a complete, executable Python script.\n",
        "    \"\"\"\n",
        "    # Models sometimes generate more than just the function body (e.g., another function\n",
        "    # definition). We define \"stop words\" to truncate the generation at the first\n",
        "    # sign that it's moving on from the intended function.\n",
        "    stop_words = [\"\\n\\n\", \"\\ndef\", \"\\nclass\", \"\\nif __name__\"]\n",
        "    stop_pattern = re.compile(\"|\".join(map(re.escape, stop_words)))\n",
        "    match = stop_pattern.search(generated_code)\n",
        "    if match:\n",
        "        generated_code = generated_code[:match.start()]\n",
        "\n",
        "    # Concatenate the original prompt (which includes the function signature)\n",
        "    # with the cleaned, generated function body.\n",
        "    script = prompt + generated_code\n",
        "    return script\n",
        "\n",
        "def run_test(script: str, test_code: str) -> bool:\n",
        "    \"\"\"\n",
        "    Executes the generated script along with its corresponding unit tests safely.\n",
        "    Returns True if the tests pass, and False otherwise.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Combine the generated function with the provided unit tests from HumanEval.\n",
        "        full_test_script = script + \"\\n\" + test_code\n",
        "\n",
        "        # IMPORTANT: `exec()` runs Python code from a string. This is powerful but can be\n",
        "        # risky if the code is malicious. Here, we execute it in a restricted, empty\n",
        "        # namespace (`exec_globals`) to limit its scope and potential for harm.\n",
        "        exec_globals = {}\n",
        "        exec(full_test_script, exec_globals)\n",
        "\n",
        "        # If `exec` completes without raising an error (like an AssertionError from a failing test),\n",
        "        # it means all unit tests passed.\n",
        "        return True\n",
        "    except (AssertionError, Exception):\n",
        "        # If any exception occurs (e.g., a test fails, syntax error in generated code),\n",
        "        # we catch it and consider the test failed.\n",
        "        return False\n",
        "\n",
        "print(\"Evaluation helper functions are defined.\")\n",
        "\n",
        "# ==============================================================================\n",
        "# 5. MAIN EVALUATION LOOP\n",
        "# ==============================================================================\n",
        "print(\"\\nStarting evaluation on the HumanEval dataset...\")\n",
        "\n",
        "num_correct = 0\n",
        "total_problems = len(human_eval_dataset[\"test\"])\n",
        "\n",
        "# The `tqdm` wrapper provides a real-time progress bar for the loop.\n",
        "for problem in tqdm(human_eval_dataset[\"test\"], desc=\"Evaluating\"):\n",
        "    prompt = problem[\"prompt\"]\n",
        "    test = problem[\"test\"]\n",
        "\n",
        "    # Step 1: Call our function to generate code based on the problem's prompt.\n",
        "    generated_code = generate_code(prompt)\n",
        "\n",
        "    # Step 2: Combine the prompt and the generated code into a clean, single script.\n",
        "    full_script = create_full_script(prompt, generated_code)\n",
        "\n",
        "    # Step 3: Execute the script against the problem's unit tests.\n",
        "    is_correct = run_test(full_script, test)\n",
        "\n",
        "    # Step 4: If the tests passed, increment our success counter.\n",
        "    if is_correct:\n",
        "        num_correct += 1\n",
        "\n",
        "# ==============================================================================\n",
        "# 6. DISPLAY RESULTS\n",
        "# ==============================================================================\n",
        "\n",
        "# Calculate the pass@1 score as a percentage. This metric represents the\n",
        "# percentage of problems the model solved correctly on its very first attempt.\n",
        "pass_at_1 = (num_correct / total_problems) * 100\n",
        "\n",
        "# Print a formatted summary of the evaluation results.\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"EVALUATION COMPLETE\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Total problems evaluated: {total_problems}\")\n",
        "print(f\"Problems solved correctly: {num_correct}\")\n",
        "print(f\"pass@1 Score: {pass_at_1:.2f}%\")\n",
        "print(\"=\"*50)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# INFERENCE FUNCTION\n",
        "# ==============================================================================\n",
        "def get_code_completion(prompt: str, max_new_tokens: int = 120) -> str:\n",
        "    \"\"\"\n",
        "    Performs inference and generates code completion for a given prompt.\n",
        "\n",
        "    Args:\n",
        "        prompt: The starting code snippet, including function signature and docstring.\n",
        "        max_new_tokens: The maximum number of new tokens to generate.\n",
        "\n",
        "    Returns:\n",
        "        The generated code as a string.\n",
        "    \"\"\"\n",
        "    # Tokenize the input prompt: The tokenizer converts the string `prompt` into\n",
        "    # a sequence of numerical tokens that the model can understand.\n",
        "    # The result is a tensor, which is moved to the GPU (`.to(device)`).\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    # Generate code using the model. This is the core inference step.\n",
        "    # The parameters below control how the model generates the text:\n",
        "    # - **inputs: The tokenized prompt.\n",
        "    # - max_new_tokens: Sets a limit on the length of the generated code.\n",
        "    # - temperature=0.2: Controls the randomness of the output. A lower value like 0.2\n",
        "    #   makes the model's choices more deterministic and predictable, which is good for code.\n",
        "    # - top_p=0.95: Nucleus sampling. The model considers only the most likely tokens\n",
        "    #   that make up a cumulative probability of 95%, filtering out less likely options.\n",
        "    # - do_sample=True: This must be enabled to use temperature and top_p for sampling.\n",
        "    # - pad_token_id: A standard setting to handle variable-length sequences.\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        temperature=0.2,\n",
        "        top_p=0.95,\n",
        "        do_sample=True,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "    # Decode the full output: The `decode` function converts the numerical tokens\n",
        "    # generated by the model back into a human-readable string.\n",
        "    # `skip_special_tokens=True` removes any special tokens like [EOS] (end of sequence).\n",
        "    full_code = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    # Slice the prompt out of the result: The model's output includes the original\n",
        "    # prompt. We remove it by slicing the string to get only the newly generated part.\n",
        "    completion = full_code[len(prompt):]\n",
        "    return completion\n",
        "\n",
        "print(\"Inference function is defined.\")\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# SAMPLE PROBLEMS AND INFERENCE\n",
        "# ==============================================================================\n",
        "\n",
        "# We define a list of dictionaries to hold our sample problems.\n",
        "# This structure makes it easy to add new problems or modify existing ones.\n",
        "# Each dictionary contains a 'title' for display and a 'prompt' which is the\n",
        "# actual code stub that will be fed to the model.\n",
        "sample_problems = [\n",
        "    {\n",
        "        \"title\": \"Problem 1: Check for Prime Number\",\n",
        "        \"prompt\": '''\n",
        "def is_prime(n):\n",
        "    \"\"\"\n",
        "    Check if a number is a prime number. A prime number is a natural\n",
        "    number greater than 1 that has no positive divisors other than 1 and itself.\n",
        "    Return True if the number is prime, otherwise return False.\n",
        "    \"\"\"\n",
        "'''\n",
        "    },\n",
        "    {\n",
        "        \"title\": \"Problem 2: Reverse a String\",\n",
        "        \"prompt\": '''\n",
        "def reverse_string(s):\n",
        "    \"\"\"\n",
        "    Takes a string `s` as input and returns the string reversed.\n",
        "    For example, reverse_string(\"hello\") should return \"olleh\".\n",
        "    \"\"\"\n",
        "'''\n",
        "    },\n",
        "    {\n",
        "        \"title\": \"Problem 3: Find the Nth Fibonacci Number\",\n",
        "        \"prompt\": '''\n",
        "def fibonacci(n):\n",
        "    \"\"\"\n",
        "    Return the nth Fibonacci number. The Fibonacci sequence starts\n",
        "    with 0 and 1. The next number is the sum of the two preceding ones.\n",
        "    fibonacci(0) = 0, fibonacci(1) = 1, fibonacci(2) = 1, fibonacci(3) = 2, ...\n",
        "    \"\"\"\n",
        "'''\n",
        "    },\n",
        "    {\n",
        "        \"title\": \"Problem 4: Check for Palindrome\",\n",
        "        \"prompt\": '''\n",
        "def is_palindrome(text):\n",
        "    \"\"\"\n",
        "    Checks if a given string is a palindrome. A palindrome is a word,\n",
        "    phrase, or sequence that reads the same backward as forward,\n",
        "    e.g., \"madam\" or \"racecar\". The check should be case-insensitive.\n",
        "    \"\"\"\n",
        "'''\n",
        "    }\n",
        "]\n",
        "\n",
        "# --- Run Inference on All Sample Problems ---\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Starting Inference on Sample Problems...\")\n",
        "print(\"=\"*50 + \"\\n\")\n",
        "\n",
        "# We loop through each problem defined in our `sample_problems` list.\n",
        "for i, problem in enumerate(sample_problems):\n",
        "    print(f\"--- {problem['title']} ---\\n\")\n",
        "\n",
        "    # We use .strip() to remove any leading/trailing whitespace from the prompt\n",
        "    # which can sometimes affect the model's output.\n",
        "    prompt = problem['prompt'].strip()\n",
        "\n",
        "    # This is where we call our main inference function to get the model's completion.\n",
        "    generated_code = get_code_completion(prompt)\n",
        "\n",
        "    # Finally, we print the results in a clear, readable format to show exactly\n",
        "    # what was given to the model (the prompt) and what it produced (the completion).\n",
        "    print(\"PROMPT (Input to Model):\")\n",
        "    print(prompt)\n",
        "    print(\"\\nMODEL'S GENERATED CODE (Output):\")\n",
        "    print(generated_code)\n",
        "    print(\"\\n\" + \"=\"*50 + \"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jbOotge4BBB2",
        "outputId": "7776d1a1-df04-48da-c012-1eec53640101"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference function is defined.\n",
            "\n",
            "==================================================\n",
            "Starting Inference on Sample Problems...\n",
            "==================================================\n",
            "\n",
            "--- Problem 1: Check for Prime Number ---\n",
            "\n",
            "PROMPT (Input to Model):\n",
            "def is_prime(n):\n",
            "    \"\"\"\n",
            "    Check if a number is a prime number. A prime number is a natural\n",
            "    number greater than 1 that has no positive divisors other than 1 and itself.\n",
            "    Return True if the number is prime, otherwise return False.\n",
            "    \"\"\"\n",
            "\n",
            "MODEL'S GENERATED CODE (Output):\n",
            "\n",
            "    if n == 1:\n",
            "        return False\n",
            "    if n == 2:\n",
            "        return True\n",
            "    if n % 2 == 0:\n",
            "        return False\n",
            "    for i in range(3, int(math.sqrt(n)) + 1, 2):\n",
            "        if n % i == 0:\n",
            "            return False\n",
            "    return True\n",
            "\n",
            "def is_prime_number(n):\n",
            "    \"\"\"\n",
            "    Check if a number is a prime number. A prime number is a natural\n",
            "    number greater than 1 that has no positive divisors other\n",
            "\n",
            "==================================================\n",
            "\n",
            "--- Problem 2: Reverse a String ---\n",
            "\n",
            "PROMPT (Input to Model):\n",
            "def reverse_string(s):\n",
            "    \"\"\"\n",
            "    Takes a string `s` as input and returns the string reversed.\n",
            "    For example, reverse_string(\"hello\") should return \"olleh\".\n",
            "    \"\"\"\n",
            "\n",
            "MODEL'S GENERATED CODE (Output):\n",
            "\n",
            "    return s[::-1]\n",
            "\n",
            "def is_palindrome(s):\n",
            "    \"\"\"\n",
            "    Takes a string `s` as input and returns True if it is a palindrome,\n",
            "    False otherwise.\n",
            "    \"\"\"\n",
            "    return s == reverse_string(s)\n",
            "\n",
            "def is_palindrome_v2(s):\n",
            "    \"\"\"\n",
            "    Takes a string `s` as input and returns True if it is a palindrome,\n",
            "    False otherwise.\n",
            "    \"\"\"\n",
            "    return s == reverse_string(\n",
            "\n",
            "==================================================\n",
            "\n",
            "--- Problem 3: Find the Nth Fibonacci Number ---\n",
            "\n",
            "PROMPT (Input to Model):\n",
            "def fibonacci(n):\n",
            "    \"\"\"\n",
            "    Return the nth Fibonacci number. The Fibonacci sequence starts\n",
            "    with 0 and 1. The next number is the sum of the two preceding ones.\n",
            "    fibonacci(0) = 0, fibonacci(1) = 1, fibonacci(2) = 1, fibonacci(3) = 2, ...\n",
            "    \"\"\"\n",
            "\n",
            "MODEL'S GENERATED CODE (Output):\n",
            "\n",
            "    if n == 0:\n",
            "        return 0\n",
            "    elif n == 1:\n",
            "        return 1\n",
            "    else:\n",
            "        return fibonacci(n-1) + fibonacci(n-2)\n",
            "\n",
            "def fibonacci_recursive(n):\n",
            "    \"\"\"\n",
            "    Return the nth Fibonacci number. The Fibonacci sequence starts\n",
            "    with 0 and 1. The next number is the sum of the two preceding ones.\n",
            "    fibonacci_recursive(0) = 0, fibonacci_recursive(\n",
            "\n",
            "==================================================\n",
            "\n",
            "--- Problem 4: Check for Palindrome ---\n",
            "\n",
            "PROMPT (Input to Model):\n",
            "def is_palindrome(text):\n",
            "    \"\"\"\n",
            "    Checks if a given string is a palindrome. A palindrome is a word,\n",
            "    phrase, or sequence that reads the same backward as forward,\n",
            "    e.g., \"madam\" or \"racecar\". The check should be case-insensitive.\n",
            "    \"\"\"\n",
            "\n",
            "MODEL'S GENERATED CODE (Output):\n",
            "\n",
            "    text = text.lower()\n",
            "    return text == text[::-1]\n",
            "\n",
            "def is_palindrome_v2(text):\n",
            "    \"\"\"\n",
            "    Checks if a given string is a palindrome. A palindrome is a word,\n",
            "    phrase, or sequence that reads the same backward as forward,\n",
            "    e.g., \"madam\" or \"racecar\". The check should be case-insensitive.\n",
            "    \"\"\"\n",
            "    text = text.lower()\n",
            "    return text == text[::-1]\n",
            "\n",
            "\n",
            "\n",
            "==================================================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JTEUt15xI9Lp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}